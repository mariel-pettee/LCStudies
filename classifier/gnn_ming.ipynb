{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "attended-jacob",
   "metadata": {},
   "source": [
    "# Graph Neural Networks with new dataset\n",
    "\n",
    "Ming Fong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c4b7013-8035-4b30-a3d0-12f23b9e9086",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "social-remainder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU'),\n",
       " PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import libraries and some constants\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize, LogNorm\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import uproot3 as ur\n",
    "import tensorflow as tf\n",
    "import atlas_mpl_style as ampl\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from tqdm import tqdm\n",
    "\n",
    "from graph_nets import blocks\n",
    "from graph_nets import graphs\n",
    "from graph_nets import modules\n",
    "from graph_nets import utils_np\n",
    "from graph_nets import utils_tf\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "import functools\n",
    "import networkx as nx\n",
    "import sonnet as snt\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.max_rows', 20)\n",
    "\n",
    "# ampl.use_atlas_style()\n",
    "params = {'legend.fontsize': 13,\n",
    "          'axes.labelsize': 18}\n",
    "plt.rcParams.update(params)\n",
    "\n",
    "# path_prefix = '/AL/Phd/maxml/'\n",
    "# plotpath = path_prefix+'caloml-atlas/inputs/Plots/'\n",
    "# modelpath = path_prefix+'caloml-atlas/classifier/Models/'\n",
    "# %config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "# # metadata\n",
    "# layers = [\"EMB1\", \"EMB2\", \"EMB3\", \"TileBar0\", \"TileBar1\", \"TileBar2\"]\n",
    "# cell_size_phi = [0.098, 0.0245, 0.0245, 0.1, 0.1, 0.1]\n",
    "# cell_size_eta = [0.0031, 0.025, 0.05, 0.1, 0.1, 0.2]\n",
    "# len_phi = [4, 16, 16, 4, 4, 4]\n",
    "# len_eta = [128, 16, 8, 4, 4, 2]\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=24220)]) #in MB\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "\n",
    "\n",
    "print(\"Num GPUs Available: \", len(gpus))\n",
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "supposed-afghanistan",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputpath = \"/clusterfs/ml4hep/mfong/ML4Pions/MLTreeData/\"\n",
    "inputpath_pi0 = inputpath + \"user.angerami.mc16_13TeV.900246.PG_singlepi0_logE0p2to2000.e8312_e7400_s3170_r12383.v01-45-gaa27bcb_OutputStream/\"\n",
    "inputpath_pion = inputpath + \"user.angerami.mc16_13TeV.900247.PG_singlepion_logE0p2to2000.e8312_e7400_s3170_r12383.v01-45-gaa27bcb_OutputStream/\"\n",
    "\n",
    "branches = ['runNumber', 'eventNumber', 'truthE', 'truthPt', 'truthEta',\n",
    "            'truthPhi', 'clusterIndex', 'nCluster', 'clusterE',\n",
    "            'clusterECalib', 'clusterPt', 'clusterEta', 'clusterPhi',\n",
    "            'cluster_nCells', 'cluster_sumCellE', 'cluster_ENG_CALIB_TOT',\n",
    "            'cluster_ENG_CALIB_OUT_T', 'cluster_ENG_CALIB_DEAD_TOT',\n",
    "            'cluster_EM_PROBABILITY', 'cluster_HAD_WEIGHT',\n",
    "            'cluster_OOC_WEIGHT', 'cluster_DM_WEIGHT', 'cluster_CENTER_MAG',\n",
    "            'cluster_FIRST_ENG_DENS', 'cluster_cell_dR_min',\n",
    "            'cluster_cell_dR_max', 'cluster_cell_dEta_min',\n",
    "            'cluster_cell_dEta_max', 'cluster_cell_dPhi_min',\n",
    "            'cluster_cell_dPhi_max', 'cluster_cell_centerCellEta',\n",
    "            'cluster_cell_centerCellPhi', 'cluster_cell_centerCellLayer',\n",
    "            'cluster_cellE_norm']\n",
    "geo_branches = [\n",
    "    'cell_geo_ID', 'cell_geo_sampling', 'cell_geo_eta', 'cell_geo_phi',\n",
    "    'cell_geo_rPerp', 'cell_geo_deta', 'cell_geo_dphi', 'cell_geo_volume',\n",
    "    'cell_geo_sigma'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "resistant-christian",
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = ur.open(inputpath_pi0 + 'user.angerami.24559740.OutputStream._000011.root')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "discrete-commonwealth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>cell_geo_ID</th>\n",
       "      <th>cell_geo_sampling</th>\n",
       "      <th>cell_geo_eta</th>\n",
       "      <th>cell_geo_phi</th>\n",
       "      <th>cell_geo_rPerp</th>\n",
       "      <th>cell_geo_deta</th>\n",
       "      <th>cell_geo_dphi</th>\n",
       "      <th>cell_geo_volume</th>\n",
       "      <th>cell_geo_sigma</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entry</th>\n",
       "      <th>subentry</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"11\" valign=\"top\">0</th>\n",
       "      <th>0</th>\n",
       "      <td>740294656</td>\n",
       "      <td>6</td>\n",
       "      <td>-2.559710</td>\n",
       "      <td>0.053900</td>\n",
       "      <td>617.735962</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.098175</td>\n",
       "      <td>1.697610e+06</td>\n",
       "      <td>49.457161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>740294658</td>\n",
       "      <td>6</td>\n",
       "      <td>-2.559648</td>\n",
       "      <td>0.151909</td>\n",
       "      <td>617.774719</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.098175</td>\n",
       "      <td>1.697610e+06</td>\n",
       "      <td>49.457161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>740294660</td>\n",
       "      <td>6</td>\n",
       "      <td>-2.559603</td>\n",
       "      <td>0.249912</td>\n",
       "      <td>617.803223</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.098175</td>\n",
       "      <td>1.697610e+06</td>\n",
       "      <td>49.457161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>740294662</td>\n",
       "      <td>6</td>\n",
       "      <td>-2.559574</td>\n",
       "      <td>0.347912</td>\n",
       "      <td>617.821167</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.098175</td>\n",
       "      <td>1.697610e+06</td>\n",
       "      <td>49.457161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>740294664</td>\n",
       "      <td>6</td>\n",
       "      <td>-2.559562</td>\n",
       "      <td>0.445909</td>\n",
       "      <td>617.828552</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.098175</td>\n",
       "      <td>1.697610e+06</td>\n",
       "      <td>49.457161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187645</th>\n",
       "      <td>1284491536</td>\n",
       "      <td>15</td>\n",
       "      <td>0.958372</td>\n",
       "      <td>-0.049087</td>\n",
       "      <td>3215.000000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.098175</td>\n",
       "      <td>1.346147e+07</td>\n",
       "      <td>20.233513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187646</th>\n",
       "      <td>1284491824</td>\n",
       "      <td>17</td>\n",
       "      <td>1.058902</td>\n",
       "      <td>-0.049087</td>\n",
       "      <td>2809.000000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.098175</td>\n",
       "      <td>1.341334e+06</td>\n",
       "      <td>10.413343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187647</th>\n",
       "      <td>1284492080</td>\n",
       "      <td>17</td>\n",
       "      <td>1.159304</td>\n",
       "      <td>-0.049087</td>\n",
       "      <td>2477.000000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.098175</td>\n",
       "      <td>1.241210e+06</td>\n",
       "      <td>10.957963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187648</th>\n",
       "      <td>1284492592</td>\n",
       "      <td>17</td>\n",
       "      <td>1.309847</td>\n",
       "      <td>-0.049087</td>\n",
       "      <td>2060.000000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.098175</td>\n",
       "      <td>7.739876e+05</td>\n",
       "      <td>12.509910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187649</th>\n",
       "      <td>1284493104</td>\n",
       "      <td>17</td>\n",
       "      <td>1.510363</td>\n",
       "      <td>-0.049087</td>\n",
       "      <td>1640.000000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.098175</td>\n",
       "      <td>4.666501e+05</td>\n",
       "      <td>11.231500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>187650 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                cell_geo_ID  cell_geo_sampling  cell_geo_eta  cell_geo_phi  \\\n",
       "entry subentry                                                               \n",
       "0     0           740294656                  6     -2.559710      0.053900   \n",
       "      1           740294658                  6     -2.559648      0.151909   \n",
       "      2           740294660                  6     -2.559603      0.249912   \n",
       "      3           740294662                  6     -2.559574      0.347912   \n",
       "      4           740294664                  6     -2.559562      0.445909   \n",
       "...                     ...                ...           ...           ...   \n",
       "      187645     1284491536                 15      0.958372     -0.049087   \n",
       "      187646     1284491824                 17      1.058902     -0.049087   \n",
       "      187647     1284492080                 17      1.159304     -0.049087   \n",
       "      187648     1284492592                 17      1.309847     -0.049087   \n",
       "      187649     1284493104                 17      1.510363     -0.049087   \n",
       "\n",
       "                cell_geo_rPerp  cell_geo_deta  cell_geo_dphi  cell_geo_volume  \\\n",
       "entry subentry                                                                  \n",
       "0     0             617.735962            0.1       0.098175     1.697610e+06   \n",
       "      1             617.774719            0.1       0.098175     1.697610e+06   \n",
       "      2             617.803223            0.1       0.098175     1.697610e+06   \n",
       "      3             617.821167            0.1       0.098175     1.697610e+06   \n",
       "      4             617.828552            0.1       0.098175     1.697610e+06   \n",
       "...                        ...            ...            ...              ...   \n",
       "      187645       3215.000000            0.1       0.098175     1.346147e+07   \n",
       "      187646       2809.000000            0.1       0.098175     1.341334e+06   \n",
       "      187647       2477.000000            0.1       0.098175     1.241210e+06   \n",
       "      187648       2060.000000            0.2       0.098175     7.739876e+05   \n",
       "      187649       1640.000000            0.2       0.098175     4.666501e+05   \n",
       "\n",
       "                cell_geo_sigma  \n",
       "entry subentry                  \n",
       "0     0              49.457161  \n",
       "      1              49.457161  \n",
       "      2              49.457161  \n",
       "      3              49.457161  \n",
       "      4              49.457161  \n",
       "...                        ...  \n",
       "      187645         20.233513  \n",
       "      187646         10.413343  \n",
       "      187647         10.957963  \n",
       "      187648         12.509910  \n",
       "      187649         11.231500  \n",
       "\n",
       "[187650 rows x 9 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geo_df = infile['CellGeo'].pandas.df(geo_branches)\n",
    "geo_df\n",
    "\n",
    "# eta and phi = pseudorapidity and azimuth angles\n",
    "# rPerp distance in mm from center\n",
    "\n",
    "# TODO try adding these later\n",
    "# cell_geo_deta is size of cell\n",
    "# cell_geo_dphi is size of cell\n",
    "# cell_geo_volume is volume of cell\n",
    "# cell_geo_sigma is noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "crucial-seafood",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/clusterfs/ml4hep/mpettee/miniconda3/envs/ming/lib/python3.8/site-packages/awkward0/array/base.py:398: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return cls.numpy.array(value, copy=False)\n"
     ]
    }
   ],
   "source": [
    "df_all_cols = infile['EventTree'].pandas.df(flatten=False)\n",
    "df = df_all_cols#[['cluster_cell_ID', 'cluster_cell_E', 'cluster_Eta', 'cluster_Phi']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designing-concord",
   "metadata": {},
   "source": [
    "## Checking events with no registered clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "continuing-democrat",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[df[\"cluster_cell_E\"].map(len) == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "spatial-commissioner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = pd.DataFrame(\n",
    "#     df_all_cols[df_all_cols[\"cluster_cell_E\"].map(len) == 0][\"truthPartStatus\"].apply(pd.Series).stack().reset_index(drop=True)\n",
    "# )\n",
    "# test.columns = [\"truthPartStatus\"]\n",
    "# test[\"truthPartE\"] = df_all_cols[df_all_cols[\"cluster_cell_E\"].map(len) == 0][\"truthPartE\"].apply(pd.Series).stack().reset_index(drop=True)\n",
    "# test = test[test[\"truthPartStatus\"] == 1]\n",
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "painful-harvest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test[\"truthPartE\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "transparent-viking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(test[\"truthPartE\"], bins = [x/100 for x in range(0, 100)], density=True)\n",
    "# plt.xlabel(\"truthPartE\")\n",
    "# plt.ylabel(\"Fraction of Events\")\n",
    "# plt.title(\"Truth Particles with Status Code 1 (No cluster)\")\n",
    "# plt.axis([0, 1, 0, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seventh-fellowship",
   "metadata": {},
   "source": [
    "# Graph making functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "oriented-newspaper",
   "metadata": {},
   "outputs": [],
   "source": [
    "def geo_df_to_xyz(geo_df):\n",
    "    \"\"\"\n",
    "    Adds xyz coordinates to geo_df\n",
    "    \n",
    "    Params:\n",
    "        geo_df: pd.DataFrame \n",
    "    \n",
    "    Returns:\n",
    "        geo_df_with_xyz: pd.DataFrame original dataframe plus x y x coordinates for each cell\n",
    "    \n",
    "    \"\"\"\n",
    "    geo_df[\"cell_geo_x\"] = geo_df[\"cell_geo_rPerp\"] * np.cos(geo_df[\"cell_geo_phi\"])\n",
    "    geo_df[\"cell_geo_y\"] = geo_df[\"cell_geo_rPerp\"] * np.sin(geo_df[\"cell_geo_phi\"])\n",
    "    cell_geo_theta = 2 * np.arctan(np.exp(-geo_df[\"cell_geo_eta\"]))\n",
    "    geo_df[\"cell_geo_z\"] = geo_df[\"cell_geo_rPerp\"] / np.tan(cell_geo_theta)\n",
    "    return geo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "explicit-underground",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_df = geo_df_to_xyz(geo_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "casual-medication",
   "metadata": {},
   "outputs": [],
   "source": [
    "# throw out empty particles in out layer []\n",
    "\n",
    "# check here for the old code of make_graph()\n",
    "# https://github.com/evilpegasus/LCStudies/blob/cnn_dev/classifier/TopoClusterClassiferGraph.ipynb\n",
    "\n",
    "def make_graph(event: pd.Series, geo_df: pd.DataFrame, is_charged):\n",
    "    \"\"\"\n",
    "    Creates a graph representation of an event\n",
    "    \n",
    "    inputs\n",
    "    event (pd.Series) one event/row from EventTree\n",
    "    geo_df (pd.DataFrame) the CellGeo DataFrame mapping cell_geo_ID to information about the cell\n",
    "    is_charged (bool) True for charged pion, False for uncharged pion\n",
    "    \n",
    "    returns\n",
    "    A pair of graph representations of the event for the GNN (train_graph, target_graph)\n",
    "    returns (None, None) if no cell energies detected\n",
    "    \"\"\"\n",
    "#     assert len(event[\"cluster_cell_E\"]) == len(event[\"cluster_cell_ID\"]), \"Error: Missmatched len of cluster_cell_E and cluster_cell_ID\"\n",
    "    # If no energies were registered return tuple(None, None)\n",
    "    if len(event[\"cluster_cell_E\"]) == 0:\n",
    "        return None, None\n",
    "    \n",
    "    temp_df = geo_df[geo_df[\"cell_geo_ID\"].isin([item for sublist in event[\"cluster_cell_ID\"] for item in sublist])]\n",
    "    temp_df = temp_df.set_index(\"cell_geo_ID\")\n",
    "    for cell_id, cell_e in zip(\n",
    "        [item for sublist in event[\"cluster_cell_ID\"] for item in sublist],\n",
    "        [item for sublist in event[\"cluster_cell_E\"] for item in sublist]\n",
    "#         np.array(event[\"cluster_cell_ID\"]).flatten(),\n",
    "#         np.array(event[\"cluster_cell_E\"]).flatten()\n",
    "    ):\n",
    "        temp_df.loc[int(cell_id), \"cell_E\"] = cell_e\n",
    "    \n",
    "    \n",
    "    \n",
    "    n_nodes = temp_df.shape[0]\n",
    "    \n",
    "    node_features = [\"cell_E\", \"cell_geo_eta\",\n",
    "                     \"cell_geo_phi\", \"cell_geo_rPerp\",\n",
    "                     \"cell_geo_deta\", \"cell_geo_dphi\",\n",
    "                     \"cell_geo_volume\"]\n",
    "    nodes = temp_df[node_features].to_numpy(dtype=np.float32).reshape(-1, len(node_features))\n",
    "    \n",
    "    # NOTE FAIR also has a faster algo for KNN search. Might want to try it\n",
    "    \n",
    "    k = 6\n",
    "    k = min(n_nodes, k)\n",
    "    \n",
    "    nbrs = NearestNeighbors(n_neighbors=k, algorithm='ball_tree').fit(temp_df[[\"cell_geo_x\", \"cell_geo_y\", \"cell_geo_z\"]])\n",
    "    distances, indices = nbrs.kneighbors(temp_df[[\"cell_geo_x\", \"cell_geo_y\", \"cell_geo_z\"]])\n",
    "    \n",
    "    senders = np.repeat([x[0] for x in indices], k-1)               # k-1 for no self edges\n",
    "    receivers = np.array([x[1:] for x in indices]).flatten()        # x[1:] for no self edges\n",
    "    edges = np.array([x[1:] for x in distances], dtype=np.float32).flatten().reshape(-1, 1)\n",
    "    n_edges = len(senders)\n",
    "    \n",
    "    global_features = [\"cluster_E\", \"cluster_Eta\", \"cluster_Phi\"]\n",
    "    global_values = np.array([x[0] for x in event[global_features]], dtype=np.float32)\n",
    "    solution = int(is_charged)\n",
    "    \n",
    "    input_datadict = {\n",
    "        \"n_node\": n_nodes,\n",
    "        \"n_edge\": n_edges,\n",
    "        \"nodes\": nodes,\n",
    "        \"edges\": edges,\n",
    "        \"senders\": senders,\n",
    "        \"receivers\": receivers,\n",
    "        \"globals\": global_values            # np.array([n_nodes], dtype=np.float32)\n",
    "    }\n",
    "    target_datadict = {\n",
    "        \"n_node\": n_nodes,\n",
    "        \"n_edge\": n_edges,\n",
    "        \"nodes\": nodes,\n",
    "        \"edges\": edges,\n",
    "        \"senders\": senders,\n",
    "        \"receivers\": receivers,\n",
    "        \"globals\": np.array([solution], dtype=np.float32)\n",
    "    }\n",
    "    input_graph = utils_tf.data_dicts_to_graphs_tuple([input_datadict])\n",
    "    target_graph = utils_tf.data_dicts_to_graphs_tuple([target_datadict])\n",
    "    \n",
    "    \n",
    "    return input_graph, target_graph\n",
    "#     return temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51ccadb4-505c-46b2-ad2a-852c458b601f",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph1, graph2 = make_graph(df.iloc[2], geo_df, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "atmospheric-determination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54.7 ms ± 552 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit make_graph(df.iloc[2], geo_df, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "hybrid-baseball",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_graphs_tuple(g, data=True):\n",
    "    for field_name in graphs.ALL_FIELDS:\n",
    "        per_replica_sample = getattr(g, field_name)\n",
    "        if per_replica_sample is None:\n",
    "            print(field_name, \"EMPTY\")\n",
    "        else:\n",
    "            print(field_name, \"has shape\", per_replica_sample.shape)\n",
    "            if data and  field_name != \"edges\":\n",
    "                print(per_replica_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "biblical-defendant",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(GraphsTuple(nodes=<tf.Tensor: shape=(254, 7), dtype=float32, numpy=\n",
       " array([[ 3.0007811e-02, -1.9909242e+00, -3.0888374e+00, ...,\n",
       "          4.1666669e-03,  9.8174773e-02,  4.4423398e+04],\n",
       "        [ 7.7241391e-02, -1.9909596e+00, -2.9905653e+00, ...,\n",
       "          4.1666669e-03,  9.8174773e-02,  4.4423398e+04],\n",
       "        [ 1.8965498e-02, -1.9950993e+00, -3.0888393e+00, ...,\n",
       "          4.1666669e-03,  9.8174773e-02,  4.4026398e+04],\n",
       "        ...,\n",
       "        [ 2.4162924e-01, -2.0597513e+00, -2.9943304e+00, ...,\n",
       "          1.0000000e-01,  9.8174773e-02,  3.8163000e+06],\n",
       "        [ 2.8949308e-01, -2.1598406e+00, -3.0925052e+00, ...,\n",
       "          1.0000000e-01,  9.8174773e-02,  3.0867100e+06],\n",
       "        [ 3.3934039e-01, -2.1598406e+00, -2.9943304e+00, ...,\n",
       "          1.0000000e-01,  9.8174773e-02,  3.0867100e+06]], dtype=float32)>, edges=<tf.Tensor: shape=(1270, 1), dtype=float32, numpy=\n",
       " array([[  4.563584],\n",
       "        [ 13.627416],\n",
       "        [ 18.128166],\n",
       "        ...,\n",
       "        [156.33746 ],\n",
       "        [267.9918  ],\n",
       "        [268.39206 ]], dtype=float32)>, receivers=<tf.Tensor: shape=(1270,), dtype=int32, numpy=array([  2,   5,   7, ..., 250, 223, 224], dtype=int32)>, senders=<tf.Tensor: shape=(1270,), dtype=int32, numpy=array([  0,   0,   0, ..., 253, 253, 253], dtype=int32)>, globals=<tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[1007.15283  ,   -2.0994453,   -3.0541317]], dtype=float32)>, n_node=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([254], dtype=int32)>, n_edge=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1270], dtype=int32)>),\n",
       " GraphsTuple(nodes=<tf.Tensor: shape=(254, 7), dtype=float32, numpy=\n",
       " array([[ 3.0007811e-02, -1.9909242e+00, -3.0888374e+00, ...,\n",
       "          4.1666669e-03,  9.8174773e-02,  4.4423398e+04],\n",
       "        [ 7.7241391e-02, -1.9909596e+00, -2.9905653e+00, ...,\n",
       "          4.1666669e-03,  9.8174773e-02,  4.4423398e+04],\n",
       "        [ 1.8965498e-02, -1.9950993e+00, -3.0888393e+00, ...,\n",
       "          4.1666669e-03,  9.8174773e-02,  4.4026398e+04],\n",
       "        ...,\n",
       "        [ 2.4162924e-01, -2.0597513e+00, -2.9943304e+00, ...,\n",
       "          1.0000000e-01,  9.8174773e-02,  3.8163000e+06],\n",
       "        [ 2.8949308e-01, -2.1598406e+00, -3.0925052e+00, ...,\n",
       "          1.0000000e-01,  9.8174773e-02,  3.0867100e+06],\n",
       "        [ 3.3934039e-01, -2.1598406e+00, -2.9943304e+00, ...,\n",
       "          1.0000000e-01,  9.8174773e-02,  3.0867100e+06]], dtype=float32)>, edges=<tf.Tensor: shape=(1270, 1), dtype=float32, numpy=\n",
       " array([[  4.563584],\n",
       "        [ 13.627416],\n",
       "        [ 18.128166],\n",
       "        ...,\n",
       "        [156.33746 ],\n",
       "        [267.9918  ],\n",
       "        [268.39206 ]], dtype=float32)>, receivers=<tf.Tensor: shape=(1270,), dtype=int32, numpy=array([  2,   5,   7, ..., 250, 223, 224], dtype=int32)>, senders=<tf.Tensor: shape=(1270,), dtype=int32, numpy=array([  0,   0,   0, ..., 253, 253, 253], dtype=int32)>, globals=<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[1.]], dtype=float32)>, n_node=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([254], dtype=int32)>, n_edge=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1270], dtype=int32)>))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_graph_pair = make_graph(df.iloc[2], geo_df, 1)\n",
    "test_graph_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "miniature-family",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nodes has shape (254, 7)\n",
      "edges has shape (1270, 1)\n",
      "receivers has shape (1270,)\n",
      "senders has shape (1270,)\n",
      "globals has shape (1, 3)\n",
      "n_node has shape (1,)\n",
      "n_edge has shape (1,)\n"
     ]
    }
   ],
   "source": [
    "print_graphs_tuple(test_graph_pair[0], data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "southwest-medium",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 46.04it/s]\n"
     ]
    }
   ],
   "source": [
    "# small set of 100 points for testing\n",
    "graph_list = []\n",
    "for i in tqdm(range(100)):\n",
    "    graph_list.append(make_graph(df.loc[i], geo_df, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a5f037b9-fb1e-4278-98f8-7c4ce7d209d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(GraphsTuple(nodes=<tf.Tensor: shape=(254, 7), dtype=float32, numpy=\n",
       " array([[ 3.0007811e-02, -1.9909242e+00, -3.0888374e+00, ...,\n",
       "          4.1666669e-03,  9.8174773e-02,  4.4423398e+04],\n",
       "        [ 7.7241391e-02, -1.9909596e+00, -2.9905653e+00, ...,\n",
       "          4.1666669e-03,  9.8174773e-02,  4.4423398e+04],\n",
       "        [ 1.8965498e-02, -1.9950993e+00, -3.0888393e+00, ...,\n",
       "          4.1666669e-03,  9.8174773e-02,  4.4026398e+04],\n",
       "        ...,\n",
       "        [ 2.4162924e-01, -2.0597513e+00, -2.9943304e+00, ...,\n",
       "          1.0000000e-01,  9.8174773e-02,  3.8163000e+06],\n",
       "        [ 2.8949308e-01, -2.1598406e+00, -3.0925052e+00, ...,\n",
       "          1.0000000e-01,  9.8174773e-02,  3.0867100e+06],\n",
       "        [ 3.3934039e-01, -2.1598406e+00, -2.9943304e+00, ...,\n",
       "          1.0000000e-01,  9.8174773e-02,  3.0867100e+06]], dtype=float32)>, edges=<tf.Tensor: shape=(1270, 1), dtype=float32, numpy=\n",
       " array([[  4.563584],\n",
       "        [ 13.627416],\n",
       "        [ 18.128166],\n",
       "        ...,\n",
       "        [156.33746 ],\n",
       "        [267.9918  ],\n",
       "        [268.39206 ]], dtype=float32)>, receivers=<tf.Tensor: shape=(1270,), dtype=int32, numpy=array([  2,   5,   7, ..., 250, 223, 224], dtype=int32)>, senders=<tf.Tensor: shape=(1270,), dtype=int32, numpy=array([  0,   0,   0, ..., 253, 253, 253], dtype=int32)>, globals=<tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[1007.15283  ,   -2.0994453,   -3.0541317]], dtype=float32)>, n_node=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([254], dtype=int32)>, n_edge=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1270], dtype=int32)>),\n",
       " GraphsTuple(nodes=<tf.Tensor: shape=(254, 7), dtype=float32, numpy=\n",
       " array([[ 3.0007811e-02, -1.9909242e+00, -3.0888374e+00, ...,\n",
       "          4.1666669e-03,  9.8174773e-02,  4.4423398e+04],\n",
       "        [ 7.7241391e-02, -1.9909596e+00, -2.9905653e+00, ...,\n",
       "          4.1666669e-03,  9.8174773e-02,  4.4423398e+04],\n",
       "        [ 1.8965498e-02, -1.9950993e+00, -3.0888393e+00, ...,\n",
       "          4.1666669e-03,  9.8174773e-02,  4.4026398e+04],\n",
       "        ...,\n",
       "        [ 2.4162924e-01, -2.0597513e+00, -2.9943304e+00, ...,\n",
       "          1.0000000e-01,  9.8174773e-02,  3.8163000e+06],\n",
       "        [ 2.8949308e-01, -2.1598406e+00, -3.0925052e+00, ...,\n",
       "          1.0000000e-01,  9.8174773e-02,  3.0867100e+06],\n",
       "        [ 3.3934039e-01, -2.1598406e+00, -2.9943304e+00, ...,\n",
       "          1.0000000e-01,  9.8174773e-02,  3.0867100e+06]], dtype=float32)>, edges=<tf.Tensor: shape=(1270, 1), dtype=float32, numpy=\n",
       " array([[  4.563584],\n",
       "        [ 13.627416],\n",
       "        [ 18.128166],\n",
       "        ...,\n",
       "        [156.33746 ],\n",
       "        [267.9918  ],\n",
       "        [268.39206 ]], dtype=float32)>, receivers=<tf.Tensor: shape=(1270,), dtype=int32, numpy=array([  2,   5,   7, ..., 250, 223, 224], dtype=int32)>, senders=<tf.Tensor: shape=(1270,), dtype=int32, numpy=array([  0,   0,   0, ..., 253, 253, 253], dtype=int32)>, globals=<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>, n_node=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([254], dtype=int32)>, n_edge=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1270], dtype=int32)>))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_graph(df.iloc[2], geo_df, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "miniature-heaven",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print graphs out\n",
    "\n",
    "# for i, g in enumerate(graph_list):\n",
    "#     print(\"Graph\", i)\n",
    "#     if g[0] is None:\n",
    "#         print(g)\n",
    "#         continue\n",
    "#     print_graphs_tuple(g[0], data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "median-dryer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a graph\n",
    "\n",
    "# graphs_nx = utils_np.graphs_tuple_to_networkxs(graph_list[2][0])\n",
    "# _, axs = plt.subplots(ncols=2, figsize=(20, 10))\n",
    "# for iax, (graph_nx, ax) in enumerate(zip(graphs_nx, axs)):\n",
    "#     nx.draw(graph_nx, ax=ax)\n",
    "#     ax.set_title(\"Graph {}\".format(iax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "front-brake",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi0_files = os.listdir(inputpath_pi0)[:400]\n",
    "pion_files = os.listdir(inputpath_pion)[:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consistent-korean",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_list = []    # list of outputs from make_graph() in the form (input graph, target graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitting-stewart",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vietnamese-pepper",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(pi0_dir, pion_dir):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "macro-beads",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi0_files = os.listdir(inputpath_pi0)\n",
    "pion_files = os.listdir(inputpath_pion)\n",
    "for i, zipped in tqdm(enumerate(zip(pi0_files, pion_files)), position=0, leave=True, total=len(pi0_files)):\n",
    "    pi0_file, pion_file = zipped\n",
    "    output_list = []\n",
    "    \n",
    "    # make pi0\n",
    "    infile = ur.open(inputpath_pi0 + pi0_file)\n",
    "    geo_df = infile['CellGeo'].pandas.df(geo_branches)\n",
    "    geo_df = geo_df_to_xyz(geo_df)\n",
    "    df = infile['EventTree'].pandas.df(flatten=False)\n",
    "    for j in tqdm(range(len(df)), position=0, leave=True):\n",
    "        output_list.append((make_graph(df.iloc[j], geo_df, 0)))\n",
    "    \n",
    "    # make pion\n",
    "    infile = ur.open(inputpath_pion + pion_file)\n",
    "    geo_df = infile['CellGeo'].pandas.df(geo_branches)\n",
    "    geo_df = geo_df_to_xyz(geo_df)\n",
    "    df = infile['EventTree'].pandas.df(flatten=False)\n",
    "    for j in tqdm(range(len(df)), position=0, leave=True):\n",
    "        output_list.append((make_graph(df.iloc[j], geo_df, 1)))\n",
    "    \n",
    "    # shuffle and dump to pickle file \n",
    "    output_list = shuffle(output_list, random_state=42)\n",
    "    with open(f'graph_data/{i}.pickle', 'wb') as handle:\n",
    "        pickle.dump(output_list, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "found-budapest",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'graph_data/{i}.pickle', 'wb') as handle:\n",
    "    pickle.dump(output_list, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "universal-video",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'graph_data/{i}.pickle', 'rb') as handle:\n",
    "    unserialized_data = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seasonal-glance",
   "metadata": {},
   "outputs": [],
   "source": [
    "unserialized_data[5][1].globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naughty-mercury",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and append pi0 graphs to graph_list\n",
    "\n",
    "for pi0_file in tqdm(pi0_files, position=0, leave=True):\n",
    "    infile = ur.open(inputpath_pi0 + pi0_file)\n",
    "    \n",
    "    geo_df = infile['CellGeo'].pandas.df(geo_branches)\n",
    "    geo_df = geo_df_to_xyz(geo_df)\n",
    "    \n",
    "    df = infile['EventTree'].pandas.df(flatten=False)\n",
    "    \n",
    "    for i in tqdm(range(len(df)), position=0, leave=True):\n",
    "        graph_list.append((make_graph(df.iloc[i], geo_df, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optical-national",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and append piplus/piminus graphs to graph_list\n",
    "\n",
    "for pion_file in tqdm(pion_files, position=0, leave=True):\n",
    "    infile = ur.open(inputpath_pion + pion_file)\n",
    "    \n",
    "    geo_df = infile['CellGeo'].pandas.df(geo_branches)\n",
    "    geo_df = geo_df_to_xyz(geo_df)\n",
    "    \n",
    "    df = infile['EventTree'].pandas.df(flatten=False)\n",
    "    \n",
    "    for i in tqdm(range(len(df)), position=0, leave=True):\n",
    "        graph_list.append((make_graph(df.iloc[i], geo_df, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifteen-alliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(graph_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verified-athens",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_list[1883620][1].globals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prompt-programming",
   "metadata": {},
   "source": [
    "# Graph Net Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jewish-water",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_LAYERS = 2\n",
    "DROPOUT_RATE = 0.1\n",
    "def make_mlp_model():\n",
    "  \"\"\"Instantiates a new MLP, followed by LayerNorm.\n",
    "\n",
    "  The parameters of each new MLP are not shared with others generated by\n",
    "  this function.\n",
    "\n",
    "  Returns:\n",
    "    A Sonnet module which contains the MLP and LayerNorm.\n",
    "  \"\"\"\n",
    "  # the activation function choices:\n",
    "  # swish, relu, relu6, leaky_relu\n",
    "  return snt.Sequential([\n",
    "#       snt.nets.MLP([128, 64]*NUM_LAYERS,\n",
    "    snt.nets.MLP([128, 64],\n",
    "        activation=tf.nn.relu,\n",
    "        activate_final=True,\n",
    "#         dropout_rate=DROPOUT_RATE\n",
    "    ),\n",
    "    snt.LayerNorm(axis=-1, create_scale=True, create_offset=False)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rising-integration",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPGraphNetwork(snt.Module):\n",
    "    \"\"\"GraphIndependent with MLP edge, node, and global models.\"\"\"\n",
    "    def __init__(self, name=\"MLPGraphNetwork\"):\n",
    "        super(MLPGraphNetwork, self).__init__(name=name)\n",
    "        self._network = modules.GraphNetwork(\n",
    "            edge_model_fn=make_mlp_model,\n",
    "            node_model_fn=make_mlp_model,\n",
    "            global_model_fn=make_mlp_model\n",
    "            )\n",
    "\n",
    "    def __call__(self, inputs,\n",
    "            edge_model_kwargs=None,\n",
    "            node_model_kwargs=None,\n",
    "            global_model_kwargs=None):\n",
    "        return self._network(inputs,\n",
    "                      edge_model_kwargs=edge_model_kwargs,\n",
    "                      node_model_kwargs=node_model_kwargs,\n",
    "                      global_model_kwargs=global_model_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improved-trigger",
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_SIZE = 128\n",
    "NUM_LAYERS = 3\n",
    "\n",
    "class GlobalClassifier(snt.Module):\n",
    "\n",
    "    def __init__(self, name=\"GlobalClassifier\"):\n",
    "        super(GlobalClassifier, self).__init__(name=name)\n",
    "\n",
    "        self._edge_block = blocks.EdgeBlock(\n",
    "            edge_model_fn=make_mlp_model,\n",
    "            use_edges = True,\n",
    "            use_receiver_nodes = True,\n",
    "            use_sender_nodes = True,\n",
    "            use_globals = True,\n",
    "            name='edge_encoder_block'\n",
    "        )\n",
    "\n",
    "        self._node_encoder_block = blocks.NodeBlock(\n",
    "            node_model_fn=make_mlp_model,\n",
    "            use_received_edges=True,\n",
    "            use_sent_edges=True,\n",
    "            use_nodes=True,\n",
    "            use_globals=True,\n",
    "            name='node_encoder_block'\n",
    "        )\n",
    "\n",
    "        self._global_block = blocks.GlobalBlock(\n",
    "            global_model_fn=make_mlp_model,\n",
    "            use_edges=True,\n",
    "            use_nodes=True,\n",
    "            use_globals=True,\n",
    "        )\n",
    "\n",
    "        self._core = MLPGraphNetwork()\n",
    "        # Transforms the outputs into appropriate shapes.\n",
    "        global_output_size = 1\n",
    "        global_fn = lambda: snt.Sequential([\n",
    "#             snt.nets.MLP([32, 64],),\n",
    "#             snt.nets.MLP([128, 64],),\n",
    "            snt.nets.MLP([128, 64, 128, global_output_size], name='global_output'),\n",
    "            tf.sigmoid\n",
    "        ])\n",
    "\n",
    "        self._output_transform = modules.GraphIndependent(None, None, global_fn)\n",
    "\n",
    "    def __call__(self, input_op, num_processing_steps):\n",
    "        latent = self._global_block(self._edge_block(self._node_encoder_block(input_op)))\n",
    "        latent0 = latent\n",
    "\n",
    "        output_ops = []\n",
    "        for _ in range(num_processing_steps):\n",
    "            core_input = utils_tf.concat([latent0, latent], axis=1)\n",
    "            latent = self._core(core_input)\n",
    "            output_ops.append(self._output_transform(latent))\n",
    "\n",
    "        return output_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "significant-arizona",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GlobalClassifier()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ready-jungle",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_graph_pair[1].globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arranged-seeking",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_graphs = model(test_graph_pair[0], 2)\n",
    "output_graphs[-1].globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elementary-addition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function:\n",
    "\n",
    "class GlobalLoss:\n",
    "    def __init__(self, real_global_weight, fake_global_weight):\n",
    "        self.w_global_real = real_global_weight\n",
    "        self.w_global_fake = fake_global_weight\n",
    "\n",
    "    def __call__(self, target_op, output_ops):\n",
    "        global_weights = target_op.globals * self.w_global_real \\\n",
    "            + (1 - target_op.globals) * self.w_global_fake\n",
    "        \n",
    "#         print(global_weights)\n",
    "        \n",
    "        loss_ops = [\n",
    "            tf.compat.v1.losses.log_loss(target_op.globals, output_op.globals, weights=global_weights)\n",
    "            for output_op in output_ops\n",
    "        ]\n",
    "        return tf.stack(loss_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joint-workplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function_global = GlobalLoss(real_global_weight = 1.0, fake_global_weight = 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compound-girlfriend",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function_global(test_graph_pair[1], output_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valued-basket",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_signature(dataset, batch_size):\n",
    "    \"\"\"\n",
    "    Get signature of inputs for the training loop.\n",
    "    The signature is used by tf.function\n",
    "    \"\"\"\n",
    "\n",
    "    input_list = []\n",
    "    target_list = []\n",
    "    for _, data in dataset.iterrows():\n",
    "        dd = make_graph(data, geo_df, 0)\n",
    "        if dd[0] is not None:\n",
    "            input_list.append(dd[0])\n",
    "            target_list.append(dd[1])\n",
    "            \n",
    "        if len(input_list) == batch_size:\n",
    "            break\n",
    "\n",
    "    inputs = utils_tf.concat(input_list, axis=0)\n",
    "    targets = utils_tf.concat(target_list, axis=0)\n",
    "    input_signature = (\n",
    "      utils_tf.specs_from_graphs_tuple(inputs),\n",
    "      utils_tf.specs_from_graphs_tuple(targets)\n",
    "    )\n",
    "    \n",
    "    return input_signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imperial-witch",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 200\n",
    "input_signature = get_signature(df, batch_size)\n",
    "\n",
    "\n",
    "# Model parameters.\n",
    "# Number of processing (message-passing) steps.\n",
    "num_processing_steps_tr = 2\n",
    "num_processing_steps_te = 2\n",
    "\n",
    "\n",
    "learning_rate = 1e-3\n",
    "optimizer = snt.optimizers.Adam(learning_rate)\n",
    "\n",
    "\n",
    "# model = models.EncodeProcessDecode(edge_output_size=2, node_output_size=2)\n",
    "last_iteration = 0\n",
    "generalization_iteration = 0\n",
    "\n",
    "logged_iterations = []\n",
    "\n",
    "losses_tr = []\n",
    "corrects_tr = []\n",
    "solveds_tr = []\n",
    "\n",
    "losses_test = []\n",
    "corrects_test = []\n",
    "solveds_test = []\n",
    "\n",
    "\n",
    "@functools.partial(tf.function, input_signature=input_signature)\n",
    "def update_step(inputs_tr, targets_tr):\n",
    "    print(\"Tracing update_step\")\n",
    "    with tf.GradientTape() as tape:\n",
    "        outputs_tr = model(inputs_tr, num_processing_steps_tr)\n",
    "        loss_ops_tr = loss_function_global(targets_tr, outputs_tr)\n",
    "        loss_op_tr = tf.math.reduce_sum(loss_ops_tr) / tf.constant(num_processing_steps_tr, dtype=tf.float32)\n",
    "\n",
    "    gradients = tape.gradient(loss_op_tr, model.trainable_variables)\n",
    "    optimizer.apply(gradients, model.trainable_variables)\n",
    "    return outputs_tr, loss_op_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consistent-geneva",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and test sets\n",
    "graph_list = shuffle(graph_list, random_state=42)\n",
    "train_graphs, test_graphs = train_test_split(graph_list, test_size = 0.2, random_state = 42)\n",
    "print(\"Number of training graph pairs: \", len(train_graphs))\n",
    "print(\"Number of testing graph pairs: \", len(test_graphs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grateful-functionality",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# %%capture output\n",
    "\n",
    "for epoch in range(100, 150):\n",
    "    total_loss = 0.\n",
    "    num_batches = 0\n",
    "\n",
    "    input_list = []\n",
    "    target_list = []\n",
    "\n",
    "\n",
    "    # training dataset\n",
    "    for data in train_graphs:\n",
    "        input_tr, target_tr = data\n",
    "        if input_tr is None:\n",
    "                continue\n",
    "        input_list.append(input_tr)\n",
    "        target_list.append(target_tr)\n",
    "        if len(input_list) >= batch_size:\n",
    "            input_tr = utils_tf.concat(input_list, axis=0)\n",
    "            target_tr = utils_tf.concat(target_list, axis=0)\n",
    "\n",
    "            current_loss = update_step(input_tr, target_tr)[1].numpy()\n",
    "            total_loss += current_loss\n",
    "\n",
    "            num_batches += 1\n",
    "            input_list = []\n",
    "            target_list = []\n",
    "    loss_tr = total_loss / num_batches\n",
    "    losses_tr.append(loss_tr)\n",
    "\n",
    "\n",
    "    # testing dataset                         ***** TODO fix this *****\n",
    "    total_loss_test = 0.\n",
    "    num_batches_test = 0\n",
    "    input_list_test = []\n",
    "    target_list_test = []\n",
    "    for data in test_graphs:\n",
    "        input_test, target_test = data\n",
    "        if input_test is None:\n",
    "                continue\n",
    "        input_list_test.append(input_test)\n",
    "        target_list_test.append(target_test)\n",
    "        if len(input_list_test) >= batch_size:\n",
    "            input_test = utils_tf.concat(input_list, axis=0)\n",
    "            target_test = utils_tf.concat(target_list, axis=0)\n",
    "            output_test = model(input_test, num_processing_steps_te)\n",
    "            loss_ops_test = loss_function_global(target_test, output_test)\n",
    "            total_loss_test += tf.math.reduce_sum(loss_ops_test) / tf.constant(num_processing_steps_tr, dtype=tf.float32)\n",
    "\n",
    "\n",
    "\n",
    "            num_batches_test += 1\n",
    "            input_list_test = []\n",
    "            target_list_test = []\n",
    "    loss_test = total_loss_test / num_batches_test\n",
    "    losses_test.append(loss_test)\n",
    "\n",
    "\n",
    "    print_string = f\"Epoch: {epoch}\\tloss_tr: {loss_tr}\\tloss_test: {loss_test}\"\n",
    "    with open(\"log2.txt\", \"a\") as log:\n",
    "        log.write(print_string + \"\\n\")\n",
    "    print(print_string)\n",
    "# TODO add a checkpoint > save to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demanding-welding",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses_tr, label=\"Train\")\n",
    "plt.plot(losses_test, label=\"Test\")\n",
    "plt.legend()\n",
    "plt.title(\"Losses per epoch\");\n",
    "\n",
    "plt.savefig(\"loss_graph.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatal-groove",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the old model w/ more layers\n",
    "\n",
    "plt.plot(losses_tr, label=\"Train\")\n",
    "plt.plot(losses_test, label=\"Test\")\n",
    "plt.legend()\n",
    "plt.title(\"Losses per epoch\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charming-marble",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc roc curve and auc\n",
    "test_predictions = []\n",
    "test_truth = []\n",
    "\n",
    "input_list_test = []\n",
    "target_list_test = []\n",
    "for data in test_graphs:\n",
    "    input_test, target_test = data\n",
    "    if input_test is None:\n",
    "            continue\n",
    "    input_list_test.append(input_test)\n",
    "    target_list_test.append(target_test)\n",
    "    if len(input_list_test) >= batch_size:\n",
    "        input_test = utils_tf.concat(input_list, axis=0)\n",
    "        target_test = utils_tf.concat(target_list, axis=0)\n",
    "        output_test = model(input_test, num_processing_steps_te)\n",
    "        test_predictions.append(output_test[-1].globals)\n",
    "        test_truth.append(target_test.globals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acting-county",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this many truth and predictions\n",
    "sum([a is not None for a, b in test_graphs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frequent-richards",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc roc curve and auc\n",
    "test_predictions2 = []\n",
    "test_truth2 = []\n",
    "\n",
    "input_list_test2 = []\n",
    "target_list_test2 = []\n",
    "for data in tqdm(test_graphs):\n",
    "    input_test, target_test = data\n",
    "    if input_test is None:\n",
    "            continue\n",
    "    output_test = model(input_test, num_processing_steps_te)\n",
    "    test_predictions2.append(output_test[-1].globals)\n",
    "    test_truth2.append(target_test.globals)\n",
    "#     if len(input_list_test) >= batch_size:\n",
    "#         input_test = utils_tf.concat(input_list, axis=0)\n",
    "#         target_test = utils_tf.concat(target_list, axis=0)\n",
    "#         output_test = model(input_test, num_processing_steps_te)\n",
    "#         test_predictions.append(output_test[-1].globals)\n",
    "#         test_truth.append(target_test.globals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "textile-gardening",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist([(a-b).numpy().flatten() for a, b in zip(test_predictions2, test_truth2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "needed-turning",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.array([x.numpy() for x in test_predictions2]).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "critical-baseball",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(\n",
    "    np.array([x.numpy() for x in test_truth2]).flatten(),\n",
    "    np.array([x.numpy() for x in test_predictions2]).flatten(),\n",
    "    pos_label=1\n",
    ")\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print(\"roc_auc:\", roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "differential-transaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fpr, tpr, label = 'AUC = %0.6f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.title(\"ROC curve\");\n",
    "# plt.savefig(\"roc_curve.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generous-attack",
   "metadata": {},
   "outputs": [],
   "source": [
    "# physics roc curve (1/fpr vs tpr)\n",
    "\n",
    "plt.plot(tpr, 1/fpr, label = 'AUC = %0.6f' % roc_auc)\n",
    "plt.legend(loc = \"lower left\")\n",
    "# plt.plot([0, 1], [0, 1],'r--')\n",
    "# plt.xlim([0, 1])\n",
    "# plt.ylim([0, 1])\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.xlabel('False Positive Rate')\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"Physics ROC curve\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nasty-stuff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# roc curve by total recorded energy\n",
    "\n",
    "energy_thresholds = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "answering-vietnam",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.array([x.numpy() for x in test_predictions2]).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "skilled-result",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(fpr))\n",
    "print(len(tpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informative-laser",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.array([x.numpy() for x in test_predictions2]).flatten()[np.array([x.numpy() for x in test_truth2]).flatten() == 1], bins=100, label=\"Truth = 1\")\n",
    "plt.hist(np.array([x.numpy() for x in test_predictions2]).flatten()[np.array([x.numpy() for x in test_truth2]).flatten() == 0], alpha=0.7, bins=100, label=\"Truth = 0\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Predicted value\")\n",
    "plt.ylabel(\"Number of predictions\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neutral-tyler",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(np.array([x.numpy() for x in test_predictions2]).flatten()[np.array([x.numpy() for x in test_truth2]).flatten() == 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "under-lodging",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(np.array([x.numpy() for x in test_predictions2]).flatten()[np.array([x.numpy() for x in test_truth2]).flatten() == 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "normal-charity",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ming",
   "language": "python",
   "name": "ming"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
