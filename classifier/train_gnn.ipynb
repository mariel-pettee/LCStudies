{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aec6b8b2-4b50-4f58-a2bd-93b47eafccaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25f857fa-47aa-426d-ba9c-9f15591b8548",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generic imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize, LogNorm\n",
    "import scipy\n",
    "import uproot\n",
    "from tqdm import tqdm\n",
    "import functools\n",
    "import pickle\n",
    "from glob import glob\n",
    "\n",
    "### ML-related\n",
    "import tensorflow as tf\n",
    "import atlas_mpl_style as ampl\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import sonnet as snt\n",
    "\n",
    "### GNN-related\n",
    "from graph_nets import blocks\n",
    "from graph_nets import graphs\n",
    "from graph_nets import modules\n",
    "from graph_nets import utils_np\n",
    "from graph_nets import utils_tf\n",
    "import networkx as nx\n",
    "\n",
    "### Custom imports \n",
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff12a721-c894-42c8-a136-2b2055e80b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "### GPU Setup\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\" # pick a number between 0 & 3\n",
    "gpus = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b67efef9-19a9-48b0-ad29-869917b8d711",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Other setup \n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.max_rows', 20)\n",
    "\n",
    "params = {'legend.fontsize': 13, 'axes.labelsize': 18}\n",
    "plt.rcParams.update(params)\n",
    "\n",
    "SEED = 15\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e58505-7d96-4e9f-ae4b-2e28c89d6cc9",
   "metadata": {},
   "source": [
    "### Load graphs into a single list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da154d44-7a21-4421-bccd-e121fdd2bfc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 60.15it/s]\n"
     ]
    }
   ],
   "source": [
    "files = glob(\"../graphs/*_pion/*.pkl\")\n",
    "graph_list = [pickle.load(open(file, 'rb')) for file in tqdm(files[:10])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46400770-53d6-42e0-9585-96ae66267d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training graph pairs:  8\n",
      "Number of testing graph pairs:  2\n"
     ]
    }
   ],
   "source": [
    "# train and test sets\n",
    "train_graphs, test_graphs = train_test_split(graph_list, test_size = 0.2, random_state = SEED)\n",
    "print(\"Number of training graph pairs: \", len(train_graphs))\n",
    "print(\"Number of testing graph pairs: \", len(test_graphs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "82eecacd-b712-4219-8eb1-ffcc60240e62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[22.377983 ,  2.4487834, -2.0464356]], dtype=float32)>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_graphs[0][1][0].globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4938bb17-f128-463d-87e5-0bad9676cce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([500], dtype=int32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(test_graphs[0][3][0].n_edge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9431bb8-d7bc-41b5-8151-f9d651493461",
   "metadata": {},
   "source": [
    "# Define GNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a912c2c8-4f48-4a8f-b043-f1536c0696ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GlobalClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf74c0f-d7e1-405e-9084-cf9f4a7a8d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 200\n",
    "input_signature = get_signature(df, batch_size)\n",
    "\n",
    "# Model parameters.\n",
    "# Number of processing (message-passing) steps.\n",
    "num_processing_steps_tr = 2\n",
    "num_processing_steps_te = 2\n",
    "\n",
    "learning_rate = 1e-3\n",
    "optimizer = snt.optimizers.Adam(learning_rate)\n",
    "\n",
    "# model = models.EncodeProcessDecode(edge_output_size=2, node_output_size=2)\n",
    "last_iteration = 0\n",
    "generalization_iteration = 0\n",
    "\n",
    "logged_iterations = []\n",
    "\n",
    "losses_tr = []\n",
    "corrects_tr = []\n",
    "solveds_tr = []\n",
    "\n",
    "losses_test = []\n",
    "corrects_test = []\n",
    "solveds_test = []\n",
    "\n",
    "@functools.partial(tf.function, input_signature=input_signature)\n",
    "def update_step(inputs_tr, targets_tr):\n",
    "    print(\"Tracing update_step\")\n",
    "    with tf.GradientTape() as tape:\n",
    "        outputs_tr = model(inputs_tr, num_processing_steps_tr)\n",
    "        loss_ops_tr = loss_function_global(targets_tr, outputs_tr)\n",
    "        loss_op_tr = tf.math.reduce_sum(loss_ops_tr) / tf.constant(num_processing_steps_tr, dtype=tf.float32)\n",
    "\n",
    "    gradients = tape.gradient(loss_op_tr, model.trainable_variables)\n",
    "    optimizer.apply(gradients, model.trainable_variables)\n",
    "    return outputs_tr, loss_op_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0730cb8-ffc1-4042-8105-cf2a1550ad05",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "    total_loss = 0.\n",
    "    num_batches = 0\n",
    "\n",
    "    input_list = []\n",
    "    target_list = []\n",
    "\n",
    "    # training dataset\n",
    "    for data in train_graphs:\n",
    "        input_tr, target_tr = data\n",
    "        if input_tr is None:\n",
    "                continue\n",
    "        input_list.append(input_tr)\n",
    "        target_list.append(target_tr)\n",
    "        if len(input_list) >= batch_size:\n",
    "            input_tr = utils_tf.concat(input_list, axis=0)\n",
    "            target_tr = utils_tf.concat(target_list, axis=0)\n",
    "\n",
    "            current_loss = update_step(input_tr, target_tr)[1].numpy()\n",
    "            total_loss += current_loss\n",
    "\n",
    "            num_batches += 1\n",
    "            input_list = []\n",
    "            target_list = []\n",
    "    loss_tr = total_loss / num_batches\n",
    "    losses_tr.append(loss_tr)\n",
    "\n",
    "    # testing dataset                         ***** TODO fix this *****\n",
    "    total_loss_test = 0.\n",
    "    num_batches_test = 0\n",
    "    input_list_test = []\n",
    "    target_list_test = []\n",
    "    for data in test_graphs:\n",
    "        input_test, target_test = data\n",
    "        if input_test is None:\n",
    "                continue\n",
    "        input_list_test.append(input_test)\n",
    "        target_list_test.append(target_test)\n",
    "        if len(input_list_test) >= batch_size:\n",
    "            input_test = utils_tf.concat(input_list, axis=0)\n",
    "            target_test = utils_tf.concat(target_list, axis=0)\n",
    "            output_test = model(input_test, num_processing_steps_te)\n",
    "            loss_ops_test = loss_function_global(target_test, output_test)\n",
    "            total_loss_test += tf.math.reduce_sum(loss_ops_test) / tf.constant(num_processing_steps_tr, dtype=tf.float32)\n",
    "\n",
    "            num_batches_test += 1\n",
    "            input_list_test = []\n",
    "            target_list_test = []\n",
    "    loss_test = total_loss_test / num_batches_test\n",
    "    losses_test.append(loss_test)\n",
    "\n",
    "    print_string = f\"Epoch: {epoch}\\tloss_tr: {loss_tr}\\tloss_test: {loss_test}\"\n",
    "    with open(\"log2.txt\", \"a\") as log:\n",
    "        log.write(print_string + \"\\n\")\n",
    "    print(print_string)\n",
    "# TODO add a checkpoint > save to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b4b86b-477d-4f1e-af95-a9c32c668051",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses_tr, label=\"Train\")\n",
    "plt.plot(losses_test, label=\"Test\")\n",
    "plt.legend()\n",
    "plt.title(\"Losses per epoch\");\n",
    "# plt.savefig(\"loss_graph.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml4pions",
   "language": "python",
   "name": "ml4pions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
